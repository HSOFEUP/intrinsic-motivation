# LEARNING PROGRESS
python main.py --experiment-name "LearningProgress" --env-id "FetchReachSparse-v3" --use-gae --gamma 0.95 --hidden-size 128 --entropy-coef 0.01 --num-processes 32 --num-steps 2048 --num-env-steps 30000000 --use-tensorboard --debug --pi-lr 1e-4 --v-lr 1e-3 --dyn-lr 5e-3 --grad-norm-max 5. --add-intrinsic-reward --intrinsic-coef 1 --max-intrinsic-reward 1.0 --save-learning-progress
python main.py --experiment-name "LearningProgress" --env-id "FetchPushSparse-v3" --use-gae --gamma 0.95 --hidden-size 128 --entropy-coef 0.01 --num-processes 32 --num-steps 2048 --num-env-steps 30000000 --use-tensorboard --debug --pi-lr 1e-4 --v-lr 1e-3 --dyn-lr 5e-3 --grad-norm-max 5. --add-intrinsic-reward --intrinsic-coef 1 --max-intrinsic-reward 1.0 --save-learning-progress
python main.py --experiment-name "LearningProgress" --env-id "FetchSlideSparse-v3" --use-gae --gamma 0.95 --hidden-size 128 --entropy-coef 0.01 --num-processes 32 --num-steps 2048 --num-env-steps 30000000 --use-tensorboard --debug --pi-lr 1e-4 --v-lr 1e-3 --dyn-lr 5e-3 --grad-norm-max 5. --add-intrinsic-reward --intrinsic-coef 1 --max-intrinsic-reward 1.0 --save-learning-progress
python main.py --experiment-name "LearningProgress" --env-id "FetchPickAndPlaceSparse-v3" --use-gae --gamma 0.95 --hidden-size 128 --entropy-coef 0.01 --num-processes 32 --num-steps 2048 --num-env-steps 30000000 --use-tensorboard --debug --pi-lr 1e-4 --v-lr 1e-3 --dyn-lr 5e-3 --grad-norm-max 5. --add-intrinsic-reward --intrinsic-coef 1 --max-intrinsic-reward 1.0 --save-learning-progress
